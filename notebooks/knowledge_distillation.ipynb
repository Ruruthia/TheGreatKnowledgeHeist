{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from transformers import BertConfig\n",
    "\n",
    "from thegreatknowledgeheist.data import get_dataloaders\n",
    "from thegreatknowledgeheist.io import load_yaml\n",
    "from thegreatknowledgeheist.models import BertFactory\n",
    "from thegreatknowledgeheist.models.bert import BaseBert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, config):\n",
    "\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"train_student_loss\",\n",
    "        dirpath=f\"{config['outputs_path']}/model_checkpoints\",\n",
    "        filename=config[\"task\"] + \"-model-{epoch:02d}-{train_student_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        logger=WandbLogger(\n",
    "            save_dir=f\"{config['outputs_path']}/logs\",\n",
    "            project=\"experiments\",\n",
    "            entity=\"mma\",\n",
    "        ),\n",
    "        gpus=config[\"gpus\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataloaders[\"train\"], dataloaders[\"val\"])\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "config_path = '/home/maria/Documents/TheGreatKnowledgeHeist/configs/train_config.yaml'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 109 M \n",
      "1 | f1    | F1Score                       | 0     \n",
      "--------------------------------------------------------\n",
      "14.8 M    Trainable params\n",
      "94.7 M    Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb7b84ff3324408490b9a1110c50bc39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e4c53577e414413b183632254dc56b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b69209f9534499ba10a19587d0fff8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d00f592297e4bfebf5cafec8d3b8027"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9d44d612a564449969e3e8a62afb6ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f36be13cd72491c907d7ad80dc5748e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1713113625734d08a8cfaa9cc13c711b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0484bc0ccff24bd19988916b44c1db6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dc9626e544342a5b2033f21a739a8ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79573b7e1b0e4c15933402956610ab4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b162ba458f5642d3b257501091d4027f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fac7036c67bf43c3b3409a4fb58fa7dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58d0a5a6132f4e58a6882be6b688f8cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▃▄▅▆▆▆▇██</td></tr><tr><td>train_accuracy</td><td>▁██</td></tr><tr><td>train_f1</td><td>▁██</td></tr><tr><td>train_loss</td><td>█▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▃▃▄▅▅▆▆▇██</td></tr><tr><td>val_accuracy</td><td>▁▇████████</td></tr><tr><td>val_f1</td><td>▁▇████████</td></tr><tr><td>val_loss</td><td>█▂▁▃▂▃▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>1.0</td></tr><tr><td>train_f1</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.04541</td></tr><tr><td>trainer/global_step</td><td>159</td></tr><tr><td>val_accuracy</td><td>0.872</td></tr><tr><td>val_f1</td><td>0.86811</td></tr><tr><td>val_loss</td><td>0.38977</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">gallant-planet-34</strong>: <a href=\"https://wandb.ai/mma/Experiments/runs/3vkkymt9\" target=\"_blank\">https://wandb.ai/mma/Experiments/runs/3vkkymt9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>/home/maria/Documents/TheGreatKnowledgeHeist/out/logs/wandb/run-20220612_163614-3vkkymt9/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = load_yaml(config_path)\n",
    "dataloaders = get_dataloaders(\n",
    "    dataset_name=config[\"task\"],\n",
    "    path_to_dataset=f\"{config['dataset_path']}/{config['task']}\",\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=config[\"num_workers\"],\n",
    ")\n",
    "\n",
    "factory = BertFactory()\n",
    "\n",
    "model = factory.create_model(config[\"task\"], config=config)\n",
    "train_model(model, dataloaders, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.pooler.dense.weight\n",
      "This layer will be frozen: bert.pooler.dense.bias\n",
      "This layer will be frozen: classifier.weight\n",
      "This layer will be frozen: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "bert_config = BertConfig(\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=6,\n",
    ")\n",
    "small_model = factory.create_model(config[\"task\"], config=config, pretrained=False, bert_config=bert_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/basic-gan.html\n",
    "\n",
    "class Discriminator(pl.LightningModule):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.eps = config[\"eps\"]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_classes, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, num_classes + 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "class AdversarialKD(pl.LightningModule):\n",
    "    def __init__(self, config, teacher_model: BaseBert, student_model: BaseBert, discriminator_model: Discriminator):\n",
    "        super().__init__()\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.eps = config[\"eps\"]\n",
    "\n",
    "        self.teacher = teacher_model\n",
    "        # self.teacher.freeze()\n",
    "        self.student = student_model\n",
    "        # self.student.unfreeze()\n",
    "        self.discriminator = discriminator_model\n",
    "        # self.discriminator.unfreeze()\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.binary_entropy_loss = nn.BCELoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        student_optimizer = Adam(self.student.parameters(), lr=self.lr, eps=self.eps)\n",
    "        # TODO: add config for discriminator\n",
    "        discriminator_optimizer = Adam(self.discriminator.parameters(), lr=self.lr, eps=self.eps)\n",
    "        return [student_optimizer, discriminator_optimizer]\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        student_outputs = self.student(**inputs)\n",
    "        teacher_outputs = self.teacher(**inputs)\n",
    "        discriminator_student_outputs = self.discriminator(student_outputs[\"logits\"])\n",
    "        discriminator_teacher_outputs = self.discriminator(teacher_outputs[\"logits\"])\n",
    "        return student_outputs, teacher_outputs, discriminator_student_outputs, discriminator_teacher_outputs\n",
    "\n",
    "    def adverserial_loss(self, outputs, targets):\n",
    "        # last logit - true/false; 0 - student, 1 - teacher\n",
    "        # :, : for acronyms?\n",
    "        return self.binary_entropy_loss(nn.Sigmoid()(outputs[:, -1]), targets)\n",
    "\n",
    "    def adverserial_categories_loss(self, outputs, targets):\n",
    "        return self.cross_entropy_loss(nn.Softmax()(outputs[:, :-1]), targets)\n",
    "\n",
    "    def l1_norm(self, outputs, targets):\n",
    "        return torch.norm((outputs * targets), 1, -1).mean()\n",
    "\n",
    "    def discriminator_loss(self, discriminator_student_outputs, discriminator_teacher_outputs, targets):\n",
    "        return 1/2 * (\n",
    "            self.adverserial_loss(discriminator_student_outputs,\n",
    "                                  torch.zeros(discriminator_student_outputs.size()[0]).to('cuda'))\n",
    "            + self.adverserial_loss(discriminator_teacher_outputs,\n",
    "                                  torch.ones(discriminator_teacher_outputs.size()[0]).to('cuda'))\n",
    "            + self.adverserial_categories_loss(discriminator_student_outputs, targets)\n",
    "            + self.adverserial_categories_loss(discriminator_teacher_outputs, targets)\n",
    "        )\n",
    "\n",
    "    def student_loss(self, supervised_loss, student_outputs, teacher_outputs, discriminator_student_outputs, discriminator_teacher_outputs, targets):\n",
    "        return supervised_loss\\\n",
    "               + self.l1_norm(student_outputs, teacher_outputs)\\\n",
    "               + self.discriminator_loss(discriminator_student_outputs, discriminator_teacher_outputs, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        student_outputs, teacher_outputs, discriminator_student_outputs, discriminator_teacher_outputs = self(**batch)\n",
    "        # train student\n",
    "        if optimizer_idx == 0:\n",
    "            loss = self.student_loss(\n",
    "                student_outputs['loss'],\n",
    "                student_outputs['logits'],\n",
    "                teacher_outputs['logits'],\n",
    "                discriminator_student_outputs,\n",
    "                discriminator_teacher_outputs,\n",
    "                batch['labels'].to('cuda')\n",
    "            )\n",
    "            self.log(\"train_student_loss\", loss)\n",
    "        # train discriminator\n",
    "        elif optimizer_idx == 1:\n",
    "            loss = self.discriminator_loss(\n",
    "                discriminator_student_outputs,\n",
    "                discriminator_teacher_outputs,\n",
    "                batch['labels'].to('cuda')\n",
    "            )\n",
    "            self.log(\"train_discriminator_loss\", loss)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# num_classes:\n",
    "# swag - 4\n",
    "# amazon - 2\n",
    "# acronym - 5\n",
    "discriminator = Discriminator(config=config, num_classes=2)\n",
    "gan  = AdversarialKD(config=config, teacher_model=model, student_model=small_model, discriminator_model=discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/maria/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/maria/Documents/TheGreatKnowledgeHeist/out/model_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | teacher             | AmazonPolarityBert | 109 M \n",
      "1 | student             | AmazonPolarityBert | 67.0 M\n",
      "2 | discriminator       | Discriminator      | 133 K \n",
      "3 | cross_entropy_loss  | CrossEntropyLoss   | 0     \n",
      "4 | binary_entropy_loss | BCELoss            | 0     \n",
      "-----------------------------------------------------------\n",
      "14.9 M    Trainable params\n",
      "161 M     Non-trainable params\n",
      "176 M     Total params\n",
      "706.296   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20cc281a35ef42e2a0f27bd1e7bc3fd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8388/1544538836.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.cross_entropy_loss(nn.Softmax()(outputs[:, :-1]), targets)\n"
     ]
    },
    {
     "ename": "ReferenceError",
     "evalue": "weakly-referenced object no longer exists",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mReferenceError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [89]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [86]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dataloaders, config)\u001B[0m\n\u001B[1;32m      3\u001B[0m checkpoint_callback \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[1;32m      4\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_student_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     dirpath\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputs_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/model_checkpoints\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      8\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     11\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     12\u001B[0m     logger\u001B[38;5;241m=\u001B[39mWandbLogger(\n\u001B[1;32m     13\u001B[0m         save_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputs_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/logs\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback],\n\u001B[1;32m     20\u001B[0m )\n\u001B[0;32m---> 22\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m wandb\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:768\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    751\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 768\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:721\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 721\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n\u001B[1;32m    723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:809\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    805\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    806\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[1;32m    807\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    808\u001B[0m )\n\u001B[0;32m--> 809\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1234\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1236\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1321\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1350\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1351\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:205\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_advance_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:296\u001B[0m, in \u001B[0;36mFitLoop.on_advance_end\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    295\u001B[0m \u001B[38;5;66;03m# call train epoch end hooks\u001B[39;00m\n\u001B[0;32m--> 296\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_callback_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_train_epoch_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_train_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1634\u001B[0m, in \u001B[0;36mTrainer._call_callback_hooks\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1632\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m callable(fn):\n\u001B[1;32m   1633\u001B[0m             \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Callback]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcallback\u001B[38;5;241m.\u001B[39mstate_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1634\u001B[0m                 \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pl_module:\n\u001B[1;32m   1637\u001B[0m     \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1638\u001B[0m     pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:308\u001B[0m, in \u001B[0;36mModelCheckpoint.on_train_epoch_end\u001B[0;34m(self, trainer, pl_module)\u001B[0m\n\u001B[1;32m    306\u001B[0m monitor_candidates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_monitor_candidates(trainer)\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_every_n_epochs \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m (trainer\u001B[38;5;241m.\u001B[39mcurrent_epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_every_n_epochs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 308\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_topk_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_last_checkpoint(trainer, monitor_candidates)\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:379\u001B[0m, in \u001B[0;36mModelCheckpoint._save_topk_checkpoint\u001B[0;34m(self, trainer, monitor_candidates)\u001B[0m\n\u001B[1;32m    377\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(m)\n\u001B[1;32m    378\u001B[0m         warning_cache\u001B[38;5;241m.\u001B[39mwarn(m)\n\u001B[0;32m--> 379\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_monitor_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:651\u001B[0m, in \u001B[0;36mModelCheckpoint._save_monitor_checkpoint\u001B[0;34m(self, trainer, monitor_candidates)\u001B[0m\n\u001B[1;32m    649\u001B[0m current \u001B[38;5;241m=\u001B[39m monitor_candidates\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmonitor)\n\u001B[1;32m    650\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_monitor_top_k(trainer, current):\n\u001B[0;32m--> 651\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_best_and_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[1;32m    653\u001B[0m     epoch \u001B[38;5;241m=\u001B[39m monitor_candidates[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:702\u001B[0m, in \u001B[0;36mModelCheckpoint._update_best_and_save\u001B[0;34m(self, current, trainer, monitor_candidates)\u001B[0m\n\u001B[1;32m    697\u001B[0m     step \u001B[38;5;241m=\u001B[39m monitor_candidates[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    698\u001B[0m     rank_zero_info(\n\u001B[1;32m    699\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, global step \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124md\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmonitor\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m reached \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m0.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    700\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (best \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_model_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m0.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m), saving model to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m as top \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    701\u001B[0m     )\n\u001B[0;32m--> 702\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m del_filepath \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m filepath \u001B[38;5;241m!=\u001B[39m del_filepath:\n\u001B[1;32m    705\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mremove_checkpoint(del_filepath)\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:384\u001B[0m, in \u001B[0;36mModelCheckpoint._save_checkpoint\u001B[0;34m(self, trainer, filepath)\u001B[0m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_save_checkpoint\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.Trainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, filepath: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 384\u001B[0m     \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_weights_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_global_step_saved \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mglobal_step\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;66;03m# notify loggers\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:2450\u001B[0m, in \u001B[0;36mTrainer.save_checkpoint\u001B[0;34m(self, filepath, weights_only, storage_options)\u001B[0m\n\u001B[1;32m   2438\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_checkpoint\u001B[39m(\n\u001B[1;32m   2439\u001B[0m     \u001B[38;5;28mself\u001B[39m, filepath: _PATH, weights_only: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, storage_options: Optional[Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2440\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2442\u001B[0m \u001B[38;5;124;03m    Runs routine to create a checkpoint.\u001B[39;00m\n\u001B[1;32m   2443\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2448\u001B[0m \n\u001B[1;32m   2449\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2450\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_checkpoint_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:444\u001B[0m, in \u001B[0;36mCheckpointConnector.save_checkpoint\u001B[0;34m(self, filepath, weights_only, storage_options)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_checkpoint\u001B[39m(\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28mself\u001B[39m, filepath: _PATH, weights_only: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, storage_options: Optional[Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    436\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;124;03m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001B[39;00m\n\u001B[1;32m    438\u001B[0m \n\u001B[1;32m    439\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m        storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001B[39;00m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 444\u001B[0m     _checkpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39msave_checkpoint(_checkpoint, filepath, storage_options\u001B[38;5;241m=\u001B[39mstorage_options)\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:369\u001B[0m, in \u001B[0;36mCheckpointConnector.dump_checkpoint\u001B[0;34m(self, weights_only)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;124;03m\"\"\"Creating a model checkpoint dictionary object from various component states.\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03m    weights_only: saving model weights only\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03m    }\u001B[39;00m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    362\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\n\u001B[1;32m    364\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;66;03m# the epoch and global step are saved for compatibility but they are not relevant for restoration\u001B[39;00m\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mcurrent_epoch,\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mglobal_step\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mglobal_step,\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch-lightning_version\u001B[39m\u001B[38;5;124m\"\u001B[39m: pl\u001B[38;5;241m.\u001B[39m__version__,\n\u001B[0;32m--> 369\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_lightning_module_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloops\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_loops_state_dict(),\n\u001B[1;32m    371\u001B[0m }\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m weights_only:\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;66;03m# dump callbacks\u001B[39;00m\n\u001B[1;32m    375\u001B[0m     checkpoint[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_callbacks_state_dict()\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:458\u001B[0m, in \u001B[0;36mCheckpointConnector._get_lightning_module_state_dict\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    455\u001B[0m     metric\u001B[38;5;241m.\u001B[39mpersistent(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    456\u001B[0m     metric\u001B[38;5;241m.\u001B[39msync()\n\u001B[0;32m--> 458\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric \u001B[38;5;129;01min\u001B[39;00m metrics:\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;66;03m# sync can be a no-op (e.g. on cpu) so `unsync` would raise a user error exception if we don't check\u001B[39;00m\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m metric\u001B[38;5;241m.\u001B[39m_is_synced:\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:405\u001B[0m, in \u001B[0;36mStrategy.lightning_module_state_dict\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;124;03m\"\"\"Returns model state.\"\"\"\u001B[39;00m\n\u001B[1;32m    404\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\n\u001B[0;32m--> 405\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1318\u001B[0m, in \u001B[0;36mModule.state_dict\u001B[0;34m(self, destination, prefix, keep_vars)\u001B[0m\n\u001B[1;32m   1316\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1318\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdestination\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_vars\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_vars\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict_hooks\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m   1320\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, destination, prefix, local_metadata)\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1320\u001B[0m, in \u001B[0;36mModule.state_dict\u001B[0;34m(self, destination, prefix, keep_vars)\u001B[0m\n\u001B[1;32m   1318\u001B[0m         module\u001B[38;5;241m.\u001B[39mstate_dict(destination, prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, keep_vars\u001B[38;5;241m=\u001B[39mkeep_vars)\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict_hooks\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m-> 1320\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdestination\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1321\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hook_result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1322\u001B[0m         destination \u001B[38;5;241m=\u001B[39m hook_result\n",
      "File \u001B[0;32m~/Documents/TheGreatKnowledgeHeist/venv/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/__init__.py:354\u001B[0m, in \u001B[0;36mstate_dict_hook\u001B[0;34m(module, destination, prefix, local_metadata)\u001B[0m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m submodule_name, submodule \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_modules():\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attr_name, attr \u001B[38;5;129;01min\u001B[39;00m submodule\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 354\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mattr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mShardedTensor\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    355\u001B[0m             destination[prefix \u001B[38;5;241m+\u001B[39m submodule_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m attr_name] \u001B[38;5;241m=\u001B[39m attr\n",
      "\u001B[0;31mReferenceError\u001B[0m: weakly-referenced object no longer exists"
     ]
    }
   ],
   "source": [
    "train_model(gan, dataloaders, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}