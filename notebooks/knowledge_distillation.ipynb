{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from typing import Sequence, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from transformers import BertConfig\n",
    "\n",
    "from thegreatknowledgeheist.data import get_dataloaders\n",
    "from thegreatknowledgeheist.io import load_yaml\n",
    "from thegreatknowledgeheist.models import BertFactory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, config):\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=f\"{config['outputs_path']}/model_checkpoints\",\n",
    "        filename=config[\"task\"] + \"-model-{epoch:02d}-{val_accuracy:.2f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        logger=WandbLogger(\n",
    "            save_dir=f\"{config['outputs_path']}/logs\",\n",
    "            project=\"experiments\",\n",
    "            entity=\"mma\",\n",
    "        ),\n",
    "        gpus=config[\"gpus\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataloaders[\"train\"], dataloaders[\"val\"])\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config_path = '/pio/scratch/1/i308362/TheGreatKnowledgeHeist/configs/train_config.yaml'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "config = load_yaml(config_path)\n",
    "dataloaders = get_dataloaders(\n",
    "    dataset_name=config[\"task\"],\n",
    "    path_to_dataset=f\"{config['dataset_path']}/{config['task']}\",\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=config[\"num_workers\"],\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "swag_checkpoint_path = '/pio/scratch/1/i308362/TheGreatKnowledgeHeist/out/swag-model-epoch=03-val_f1=0.74.ckpt'\n",
    "teacher_config = BertConfig()\n",
    "student_config = BertConfig(\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=6,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from torch.optim import Adam\n",
    "\n",
    "class BaseKD(pl.LightningModule, ABC):\n",
    "    def __init__(self, config, teacher_checkpoint: str, teacher_config: BertConfig, student_config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.eps = config[\"eps\"]\n",
    "\n",
    "        factory = BertFactory()\n",
    "        teacher_config.output_hidden_states = True\n",
    "        student_config.output_hidden_states = True\n",
    "\n",
    "        self.teacher = factory.create_model(config['task'], config=config, bert_config=teacher_config, checkpoint_path=teacher_checkpoint)\n",
    "        self.teacher.freeze()\n",
    "        self.student = factory.create_model(config['task'], config=config, bert_config=student_config, pretrained=False)\n",
    "        self.student.unfreeze()\n",
    "\n",
    "    @abstractmethod\n",
    "    def logits_loss(self, student_logits, teacher_logits):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def layers_loss(self, student_layers, teacher_layers):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.student.parameters(), lr=self.lr, eps=self.eps)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        student_outputs = self.student(**inputs)\n",
    "        teacher_outputs = self.teacher(**inputs)\n",
    "        return student_outputs, teacher_outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        student_outputs, teacher_outputs = self(**batch)\n",
    "        accuracy = self.student.calculate_accuracy(student_outputs['logits'], batch[\"labels\"])\n",
    "        loss = (\n",
    "                student_outputs['loss']\n",
    "                + self.logits_loss(student_outputs['logits'], teacher_outputs['logits'])\n",
    "                + self.layers_loss(student_outputs['hidden_states'], teacher_outputs['hidden_states'])\n",
    "        )\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        student_outputs, teacher_outputs = self(**batch)\n",
    "        accuracy = self.student.calculate_accuracy(student_outputs['logits'], batch[\"labels\"])\n",
    "        loss = (\n",
    "                student_outputs['loss']\n",
    "                + self.logits_loss(student_outputs['logits'], teacher_outputs['logits'])\n",
    "                + self.layers_loss(student_outputs['hidden_states'], teacher_outputs['hidden_states'])\n",
    "        )\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_accuracy\", accuracy)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class BaselineKD(BaseKD):\n",
    "    def __init__(self, config, teacher_checkpoint: str, teacher_config: BertConfig, student_config: BertConfig):\n",
    "        super().__init__(config, teacher_checkpoint, teacher_config, student_config)\n",
    "\n",
    "    def layers_loss(self, student_layers, teacher_layers):\n",
    "        return 0\n",
    "\n",
    "    def logits_loss(self, student_logits, teacher_logits):\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax, kl_div\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class KL_div(nn.Module):\n",
    "    def __init__(self, temperature: float = 1):\n",
    "        super(KL_div, self).__init__()\n",
    "\n",
    "        self.T = temperature\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits):\n",
    "        return kl_div(\n",
    "            softmax(student_logits / self.T, dim=1),\n",
    "            softmax(teacher_logits / self.T, dim=1),\n",
    "            reduction='batchmean'\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class LogitsKD(BaseKD):\n",
    "    def __init__(self, config, teacher_checkpoint: str, teacher_config: BertConfig, student_config: BertConfig, temperature: float = 1):\n",
    "        super().__init__(config, teacher_checkpoint, teacher_config, student_config)\n",
    "\n",
    "        self.logits_criterion = KL_div(temperature)\n",
    "\n",
    "\n",
    "    def logits_loss(self, student_logits, teacher_logits):\n",
    "        return self.logits_criterion(student_logits, teacher_logits)\n",
    "\n",
    "    def layers_loss(self, student_layers, teacher_layers):\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.pooler.dense.weight\n",
      "This layer will be frozen: bert.pooler.dense.bias\n",
      "This layer will be frozen: classifier.weight\n",
      "This layer will be frozen: classifier.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33madrianurbanski\u001B[0m (\u001B[33mmma\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path /pio/scratch/1/i308362/TheGreatKnowledgeHeist/out/logs/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path /pio/scratch/1/i308362/TheGreatKnowledgeHeist/out/logs/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/tmp/wandb/run-20220612_165307-3mogijdy</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/mma/Experiments/runs/3mogijdy\" target=\"_blank\">zany-durian-37</a></strong> to <a href=\"https://wandb.ai/mma/Experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /pio/scratch/1/i308362/TheGreatKnowledgeHeist/out/model_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | teacher | SwagBert | 109 M \n",
      "1 | student | SwagBert | 67.0 M\n",
      "-------------------------------------\n",
      "67.0 M    Trainable params\n",
      "109 M     Non-trainable params\n",
      "176 M     Total params\n",
      "705.755   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57b7fa2b2105449387ac61c9cf702b98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a7781ce2ac545949142356252853fb1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dff6c1e93e9649e9891858644f1f32ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_model = BaselineKD(config, swag_checkpoint_path, teacher_config, student_config)\n",
    "train_model(baseline_model, dataloaders, config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.pooler.dense.weight\n",
      "This layer will be frozen: bert.pooler.dense.bias\n",
      "This layer will be frozen: classifier.weight\n",
      "This layer will be frozen: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "distilled_model = LogitsKD(config, swag_checkpoint_path, teacher_config, student_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /pio/scratch/1/i308362/TheGreatKnowledgeHeist/out/model_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name             | Type     | Params\n",
      "----------------------------------------------\n",
      "0 | teacher          | SwagBert | 109 M \n",
      "1 | student          | SwagBert | 67.0 M\n",
      "2 | logits_criterion | KL_div   | 0     \n",
      "----------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "109 M     Non-trainable params\n",
      "176 M     Total params\n",
      "705.755   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbbb31d7282643f8a03e3aa5b1eb5fcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de70d83940c8477e8ec775bb1c9fb5a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [37]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistilled_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dataloaders, config)\u001B[0m\n\u001B[1;32m      2\u001B[0m checkpoint_callback \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[1;32m      3\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m     dirpath\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputs_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/model_checkpoints\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     11\u001B[0m     logger\u001B[38;5;241m=\u001B[39mWandbLogger(\n\u001B[1;32m     12\u001B[0m         save_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputs_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/logs\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     18\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback],\n\u001B[1;32m     19\u001B[0m )\n\u001B[0;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m wandb\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:768\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    751\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    766\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 768\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:721\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 721\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n\u001B[1;32m    723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:809\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    805\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    806\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[1;32m    807\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    808\u001B[0m )\n\u001B[0;32m--> 809\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1234\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1236\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1321\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1350\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1351\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:268\u001B[0m, in \u001B[0;36mFitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(\n\u001B[1;32m    265\u001B[0m     dataloader, batch_to_device\u001B[38;5;241m=\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_strategy_hook, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_to_device\u001B[39m\u001B[38;5;124m\"\u001B[39m, dataloader_idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    266\u001B[0m )\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 268\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:208\u001B[0m, in \u001B[0;36mTrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 208\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    212\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001B[0m, in \u001B[0;36mTrainingBatchLoop.advance\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m     87\u001B[0m     optimizers \u001B[38;5;241m=\u001B[39m _get_active_optimizers(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizer_frequencies, batch_idx)\n\u001B[0;32m---> 88\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     90\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_loop\u001B[38;5;241m.\u001B[39mrun(split_batch, batch_idx)\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203\u001B[0m, in \u001B[0;36mOptimizerLoop.advance\u001B[0;34m(self, batch, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madvance\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: Any, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[0;32m--> 203\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_optimization\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim_progress\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_position\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001B[39;00m\n\u001B[1;32m    211\u001B[0m         \u001B[38;5;66;03m# would be skipped otherwise\u001B[39;00m\n\u001B[1;32m    212\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx] \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39masdict()\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256\u001B[0m, in \u001B[0;36mOptimizerLoop._run_optimization\u001B[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001B[0m\n\u001B[1;32m    249\u001B[0m         closure()\n\u001B[1;32m    251\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    258\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;66;03m# if no result, user decided to skip optimization\u001B[39;00m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001B[39;00m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;66;03m# TODO: find proper way to handle updating running loss\u001B[39;00m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:369\u001B[0m, in \u001B[0;36mOptimizerLoop._optimizer_step\u001B[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[1;32m    368\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 369\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_tpu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTPUAccelerator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_native_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mamp_backend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mAMPType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNATIVE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_lbfgs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_lbfgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1593\u001B[0m, in \u001B[0;36mTrainer._call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1590\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m   1592\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1593\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1595\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1596\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:1644\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001B[0m\n\u001B[1;32m   1562\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[1;32m   1563\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1564\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1571\u001B[0m     using_lbfgs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1572\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1573\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1574\u001B[0m \u001B[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1575\u001B[0m \u001B[38;5;124;03m    each optimizer.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1642\u001B[0m \n\u001B[1;32m   1643\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1644\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:168\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 168\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:193\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;124;03m\"\"\"Performs the actual optimizer step.\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \n\u001B[1;32m    185\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    192\u001B[0m model \u001B[38;5;241m=\u001B[39m model \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\n\u001B[0;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:155\u001B[0m, in \u001B[0;36mPrecisionPlugin.optimizer_step\u001B[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[1;32m    154\u001B[0m     closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/torch/optim/adam.py:100\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 100\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    103\u001B[0m     params_with_grad \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:140\u001B[0m, in \u001B[0;36mPrecisionPlugin._wrap_closure\u001B[0;34m(self, model, optimizer, optimizer_idx, closure)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    129\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    132\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m    133\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \n\u001B[1;32m    137\u001B[0m \u001B[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001B[39;00m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 140\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer, optimizer_idx)\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 148\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 134\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:427\u001B[0m, in \u001B[0;36mOptimizerLoop._training_step\u001B[0;34m(self, split_batch, batch_idx, opt_idx)\u001B[0m\n\u001B[1;32m    422\u001B[0m step_kwargs \u001B[38;5;241m=\u001B[39m _build_training_step_kwargs(\n\u001B[1;32m    423\u001B[0m     lightning_module, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, split_batch, batch_idx, opt_idx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hiddens\n\u001B[1;32m    424\u001B[0m )\n\u001B[1;32m    426\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[0;32m--> 427\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()\n\u001B[1;32m    430\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, training_step_output)\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1763\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1762\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1763\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:333\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;124;03m\"\"\"The actual training step.\u001B[39;00m\n\u001B[1;32m    329\u001B[0m \n\u001B[1;32m    330\u001B[0m \u001B[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001B[39;00m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mtrain_step_context():\n\u001B[0;32m--> 333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [24]\u001B[0m, in \u001B[0;36mBaseKD.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     37\u001B[0m student_outputs, teacher_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch)\n\u001B[1;32m     38\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstudent\u001B[38;5;241m.\u001B[39mcalculate_accuracy(student_outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m'\u001B[39m], batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     39\u001B[0m loss \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     40\u001B[0m         student_outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 41\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlogits\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mteacher_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlogits\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers_loss(student_outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_states\u001B[39m\u001B[38;5;124m'\u001B[39m], teacher_outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_states\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     43\u001B[0m )\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss)\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
      "Input \u001B[0;32mIn [35]\u001B[0m, in \u001B[0;36mLogitsKD.logits_loss\u001B[0;34m(self, student_logits, teacher_logits)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlogits_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, student_logits, teacher_logits):\n\u001B[0;32m---> 10\u001B[0m     student_logits\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     student_logits\u001B[38;5;241m.\u001B[39mretain_grad()\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(student_logits\u001B[38;5;241m.\u001B[39mgrad)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "train_model(distilled_model, dataloaders, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class LayersKD(LogitsKD):\n",
    "    def __init__(\n",
    "            self, config, teacher_checkpoint: str, teacher_config: BertConfig, student_config: BertConfig,\n",
    "            temperature: float = 1, layers_map: Optional[Sequence[int]] = None\n",
    "    ):\n",
    "        super().__init__(config, teacher_checkpoint, teacher_config, student_config, temperature)\n",
    "\n",
    "        if layers_map is None:\n",
    "            num_student_layers = self.student.model.config.num_hidden_layers\n",
    "            num_teacher_layers = self.teacher.model.config.num_hidden_layers\n",
    "            layers_map = np.linspace(0, num_teacher_layers, num=num_student_layers, endpoint=False, dtype=int)\n",
    "\n",
    "        self.layers_map = layers_map\n",
    "        self.layers_criterion = nn.MSELoss()\n",
    "\n",
    "    def layers_loss(self, student_layers, teacher_layers):\n",
    "        loss = []\n",
    "        print(student_layers[0].requires_grad)\n",
    "        print(teacher_layers[0].requires_grad)\n",
    "        for student_layer, teacher_layer in enumerate(self.layers_map):\n",
    "            loss.append(self.layers_criterion(student_layers[student_layer], teacher_layers[teacher_layer]))\n",
    "        return sum(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.6.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.7.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.8.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.9.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.8.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /pio/scratch/1/i308362/TheGreatKnowledgeHeist/out/model_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name             | Type     | Params\n",
      "----------------------------------------------\n",
      "0 | teacher          | SwagBert | 109 M \n",
      "1 | student          | SwagBert | 67.0 M\n",
      "2 | logits_criterion | KL_div   | 0     \n",
      "3 | layers_criterion | MSELoss  | 0     \n",
      "----------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "109 M     Non-trainable params\n",
      "176 M     Total params\n",
      "705.755   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This layer will be frozen: bert.embeddings.word_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.position_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.token_type_embeddings.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.weight\n",
      "This layer will be frozen: bert.embeddings.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.query.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.key.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.self.value.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.intermediate.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.dense.bias\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "This layer will be frozen: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "This layer will be frozen: bert.pooler.dense.weight\n",
      "This layer will be frozen: bert.pooler.dense.bias\n",
      "This layer will be frozen: classifier.weight\n",
      "This layer will be frozen: classifier.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9038b9f9eb44a7b8283c02f95e968fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "942c34c339154eb08392cefb276cfb74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/scratch/1/i308362/miniconda3/envs/knowledge_heist/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16ca826d33504871887a71eda290dfb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">distinctive-microwave-29</strong>: <a href=\"https://wandb.ai/mma/Experiments/runs/3e96wu89\" target=\"_blank\">https://wandb.ai/mma/Experiments/runs/3e96wu89</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>/tmp/wandb/run-20220612_145107-3e96wu89/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distilled_model = LayersKD(config, swag_checkpoint_path, teacher_config, student_config)\n",
    "train_model(distilled_model, dataloaders, config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}